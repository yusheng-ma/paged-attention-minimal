import sys
import json
import time

import torch
from flash_attn import flash_attn_with_kvcache

from tokenizer import Tokenizer

# -----------------------------
# è¨­å®šèˆ‡æ¨¡å‹è¼‰å…¥
# -----------------------------
device = 'cuda'
model_name = 'Meta-Llama-3-8B-Instruct'
tokenizer_path = f'{model_name}/original/tokenizer.model'
tokenizer = Tokenizer(model_path=tokenizer_path)

# è¼‰å…¥æ¨¡å‹æ¬Šé‡
model = torch.load(
    f'{model_name}/original/consolidated.00.pth',
    map_location=device,
    mmap=False,
    weights_only=False
)

with open(f'{model_name}/original/params.json', 'r') as f:
    config = json.load(f)

dim = config['dim']
n_layers = config['n_layers']
n_heads = config['n_heads']
n_kv_heads = config['n_kv_heads']
vocab_size = config['vocab_size']  # Llama-3: 128256
multiple_of = config['multiple_of']
ffn_dim_multiplier = config['ffn_dim_multiplier']
norm_eps = config['norm_eps']
rope_theta = torch.tensor(config['rope_theta'], device=device)
head_dim = dim // n_heads  # 128
max_seq_len = 8192

# Llama-3 Special Token IDs
LLAMA3_BEGIN_OF_TEXT = 128000
LLAMA3_START_HEADER = 128006
LLAMA3_END_HEADER = 128007
LLAMA3_EOT_ID = 128009
LLAMA3_ASSISTANT_HEADER = 128008

stop_tokens_set = {LLAMA3_EOT_ID}
stop_tokens_tensor = torch.tensor(list(stop_tokens_set), device=device)

# -----------------------------
# Embedding Layer
# -----------------------------
embedding_layer = torch.nn.Embedding(vocab_size, dim, device=device, _weight=model['tok_embeddings.weight'])

# -----------------------------
# RoPE é »ç‡é è¨ˆç®—
# -----------------------------
freqs = 1.0 / (rope_theta ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))
freqs_for_each_token = torch.outer(torch.arange(max_seq_len, device=device), freqs)
freqs_cis_max = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)

# -----------------------------
# RoPE å·¥å…·å‡½æ•¸
# -----------------------------
def reshape_for_broadcast(freqs_cis, x):
    shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)

def apply_rotary_emb(xq, xk, freqs_cis):
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)

def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights

# -----------------------------
# å‰å‘æ¨ç†å‡½æ•¸
# -----------------------------
def forward(tokens, start_pos):
    with torch.no_grad():  # ç¢ºä¿ä¸è¨ˆç®—æ¢¯åº¦
        bsz, T = tokens.shape
        assert bsz == 1, "Batch size must be 1"

        # ğŸ” å®‰å…¨æª¢æŸ¥ï¼štoken æ˜¯å¦åœ¨åˆæ³•ç¯„åœå…§
        if tokens.max() >= vocab_size or tokens.min() < 0:
            invalid = tokens[(tokens < 0) | (tokens >= vocab_size)]
            raise ValueError(f"Invalid token(s) found: {invalid.tolist()} (vocab_size={vocab_size})")

        final_embedding = embedding_layer(tokens)
        freqs_cis = freqs_cis_max[start_pos:start_pos+T]

        for layer in range(n_layers):
            q_layer = model[f'layers.{layer}.attention.wq.weight']
            k_layer = model[f'layers.{layer}.attention.wk.weight']
            v_layer = model[f'layers.{layer}.attention.wv.weight']
            w_layer = model[f'layers.{layer}.attention.wo.weight']

            layer_embedding_norm = rms_norm(final_embedding, model[f'layers.{layer}.attention_norm.weight'])

            q = layer_embedding_norm @ q_layer.T
            k = layer_embedding_norm @ k_layer.T
            v = layer_embedding_norm @ v_layer.T

            q = q.view(bsz, T, n_heads, head_dim)
            k = k.view(bsz, T, n_kv_heads, head_dim)
            v = v.view(bsz, T, n_kv_heads, head_dim)

            q, k = apply_rotary_emb(q, k, freqs_cis)

            k_cache, v_cache = kv_cache[layer]
            y = flash_attn_with_kvcache(
                q, k_cache, v_cache, k, v,
                cache_seqlens=start_pos,
                causal=True
            )
            stacked_qkv_attention = y.view(bsz, T, dim)

            embedding_delta = stacked_qkv_attention @ w_layer.T
            embedding_after_edit = final_embedding + embedding_delta

            ffn_norm = rms_norm(embedding_after_edit, model[f'layers.{layer}.ffn_norm.weight'])
            w1 = model[f'layers.{layer}.feed_forward.w1.weight']
            w2 = model[f'layers.{layer}.feed_forward.w2.weight']
            w3 = model[f'layers.{layer}.feed_forward.w3.weight']
            up = torch.functional.F.silu(ffn_norm @ w1.T)
            gate = ffn_norm @ w3.T
            output_after_feedforward = (up * gate) @ w2.T

            final_embedding = embedding_after_edit + output_after_feedforward

        final_embedding = rms_norm(final_embedding, model['norm.weight'])
        logits = final_embedding[:, -1, :] @ model['output.weight'].T
        next_token = torch.argmax(logits, dim=-1)
        return next_token

# -----------------------------
# æ‰‹å‹•æ§‹å»º Llama-3 å°è©± promptï¼ˆä¿®æ­£ç‰ˆï¼‰
# -----------------------------
def build_prompt(tokenizer, dialog):
    tokens = [LLAMA3_BEGIN_OF_TEXT]
    for message in dialog:
        role = message["role"]
        content = message["content"]
        if role == "system":
            tokens.extend([LLAMA3_START_HEADER, LLAMA3_END_HEADER])
        elif role == "user":
            tokens.extend([LLAMA3_START_HEADER, LLAMA3_END_HEADER])
        elif role == "assistant":
            tokens.extend([LLAMA3_ASSISTANT_HEADER, LLAMA3_END_HEADER])
        content_tokens = tokenizer.encode(content, bos=False, eos=False)
        tokens.extend(content_tokens)
        tokens.append(LLAMA3_EOT_ID)
    tokens.extend([LLAMA3_ASSISTANT_HEADER, LLAMA3_END_HEADER])
    return tokens

# -----------------------------
# ğŸ”¥ 5 æ¬¡ Warmup
# -----------------------------
def warmup(num_warmups=5):
    print(f"ğŸŸ¢ Warmup é–‹å§‹ï¼ˆ{num_warmups} æ¬¡ï¼‰...")
    dummy_dialog = [{"role": "user", "content": "Hello"}]
    prompt_tokens = build_prompt(tokenizer, dummy_dialog)
    tokens = torch.full((1, max_seq_len), 128012, dtype=torch.long, device=device)
    tokens[0, :len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=device)

    for i in range(num_warmups):
        print(f"  â³ Warmup {i+1}/{num_warmups}")
        # é‡ç½® KV Cache
        for layer in range(n_layers):
            kv_cache[layer][0].zero_()
            kv_cache[layer][1].zero_()
        with torch.no_grad():
            forward(tokens[:, :len(prompt_tokens)], 0)
        torch.cuda.synchronize()
    print("ğŸŸ¢ Warmup å®Œæˆ\n")

# -----------------------------
# ä¸»ç¨‹å¼
# -----------------------------
with open('my-sharegpt-filtered.json', 'r', encoding='utf-8') as f:
    sharegpt = json.load(f)

num_requests = int(sys.argv[1])
requests = []
for i in range(num_requests):
    convs = sharegpt[i]['conversations']
    if len(convs) > 0:
        requests.append({'role': 'user', 'content': convs[0]['value']})

# ğŸŸ¨ å…¨å±€ KV Cache
kv_cache = [
    (
        torch.zeros((1, max_seq_len, n_kv_heads, head_dim), dtype=torch.bfloat16, device=device),
        torch.zeros((1, max_seq_len, n_kv_heads, head_dim), dtype=torch.bfloat16, device=device)
    )
    for _ in range(n_layers)
]

# ğŸŸ¨ Prompt Cache Pool
kv_cache_pool = {}

# ğŸ”¥ Step 1: 5 æ¬¡ Warmup
warmup(num_warmups=5)

# ğŸ“Š åˆå§‹åŒ–çµ±è¨ˆè®Šæ•¸
total_prefill_time = 0.0       # ç´¯è¨ˆæ‰€æœ‰ Prefill æ™‚é–“
total_restore_time = 0.0       # ç´¯è¨ˆæ‰€æœ‰ Cache æ¢å¾©æ™‚é–“
total_saved_time = 0.0         # ç¸½å…±ç¯€çœçš„æ™‚é–“
prefill_time_baseline = None   # ç”¨æ–¼è¨ˆç®—ç¯€çœæ™‚é–“çš„åŸºæº–ï¼ˆç¬¬ä¸€æ¬¡ Prefill æ™‚é–“ï¼‰
total_fragmented_memory = 0

print(f"é–‹å§‹è™•ç† {num_requests} å€‹è«‹æ±‚ï¼ˆå‹•æ…‹æ¸¬é‡ Prefill æ™‚é–“ï¼‰...\n")

for req_idx, dialog in enumerate(requests):
    print(f"[{req_idx+1}/{num_requests}] è™•ç†ä¸­...")

    # æ§‹å»º prompt
    prompt_tokens = build_prompt(tokenizer, [dialog])
    prompt_len = len(prompt_tokens)
    prompt_key = tuple(prompt_tokens)

    # å®‰å…¨æª¢æŸ¥
    if not all(0 <= t < vocab_size for t in prompt_tokens):
        invalid = [t for t in prompt_tokens if not (0 <= t < vocab_size)]
        raise ValueError(f"Invalid token in prompt: {invalid}")

    # åˆå§‹åŒ– tokens
    pad_id = 128012
    tokens = torch.full((1, max_seq_len), pad_id, dtype=torch.long, device=device)
    tokens[0, :prompt_len] = torch.tensor(prompt_tokens, dtype=torch.long, device=device)

    input_text_mask = tokens != pad_id
    eos_reached = torch.tensor([False], device=device)
    generated_tokens = []

    # ğŸ” æª¢æŸ¥æ˜¯å¦å‘½ä¸­ cache
    if prompt_key in kv_cache_pool:
        print(f"âœ… å‘½ä¸­ç·©å­˜ï¼æ¢å¾© KV Cache (é•·åº¦: {prompt_len})...")

        # ğŸ”¥ çœŸå¯¦è¨ˆæ™‚ï¼šæ¢å¾© cache çš„æ™‚é–“
        restore_start = time.time()
        cached_kv = kv_cache_pool[prompt_key]
        for layer in range(n_layers):
            k_c, v_c = cached_kv[layer]
            # æ³¨æ„ï¼šæˆ‘å€‘åªè¤‡è£½åˆ° prompt_len é•·åº¦
            kv_cache[layer][0][:, :prompt_len] = k_c[:, :prompt_len].clone()
            kv_cache[layer][1][:, :prompt_len] = v_c[:, :prompt_len].clone()
        torch.cuda.synchronize()  # é‡è¦ï¼šåŒæ­¥ GPU
        restore_time = time.time() - restore_start
        total_restore_time += restore_time
        print(f"â±ï¸  æ¢å¾© Cache æ™‚é–“: {restore_time:.3f} ç§’")

        # è¨ˆç®—ç¯€çœæ™‚é–“ï¼ˆç›¸å°æ–¼ç¬¬ä¸€æ¬¡ Prefillï¼‰
        if prefill_time_baseline is not None:
            saved_time = prefill_time_baseline - restore_time
            total_saved_time += max(0, saved_time)  # ç¢ºä¿ä¸ç‚ºè² 
            print(f"âœ… ç¯€çœæ™‚é–“: {saved_time:.3f} ç§’")

        prev_pos = prompt_len

    else:
        print(f"ğŸ†• æ–° promptï¼ŒåŸ·è¡Œ Prefill (é•·åº¦: {prompt_len})...")
        prev_pos = 0

        # ğŸ”¥ çœŸå¯¦è¨ˆæ™‚ï¼šPrefill æ™‚é–“
        prefill_start = time.time()
        forward(tokens[:, prev_pos:prompt_len], prev_pos)
        torch.cuda.synchronize()
        prefill_time = time.time() - prefill_start
        total_prefill_time += prefill_time
        print(f"â±ï¸  Prefill æ™‚é–“: {prefill_time:.3f} ç§’")

        # è¨­å®šåŸºæº–ï¼ˆç¬¬ä¸€æ¬¡ Prefill çš„æ™‚é–“ï¼‰
        if prefill_time_baseline is None:
            prefill_time_baseline = prefill_time

        prev_pos = prompt_len

        # ğŸ”– ç·©å­˜ KV
        cached_kv = [(k.clone(), v.clone()) for k, v in kv_cache]
        kv_cache_pool[prompt_key] = cached_kv

    # âœ… é–‹å§‹ç”Ÿæˆ
    while prev_pos < max_seq_len - 1:
        cur_pos = prev_pos + 1
        next_token = forward(tokens[:, prev_pos:cur_pos], prev_pos)
        next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)
        tokens[0, cur_pos] = next_token.item()
        generated_tokens.append(next_token.item())

        if next_token.item() in stop_tokens_set:
            eos_reached[0] = True

        prev_pos = cur_pos

        if eos_reached.item():
            break

    # è§£ç¢¼ç”Ÿæˆçµæœ
    start_idx = prompt_len
    end_idx = start_idx + len(generated_tokens)
    toks = tokens[0, start_idx:end_idx].tolist()

    cleaned_tokens = []
    for t in toks:
        if t in stop_tokens_set:
            break
        cleaned_tokens.append(t)

    try:
        decoded = tokenizer.decode(cleaned_tokens)
    except Exception as e:
        decoded = f"[Decode Error: {e}]"

    print("ğŸ“ æç¤ºï¼š", dialog['content'])
    print("ğŸ’¬ å›è¦†ï¼š", decoded)
    print("-" * 80)

    # è¨ˆç®—ç¢ç‰‡è¨˜æ†¶é«”
    seq_len = prompt_len + len(cleaned_tokens)
    fragmented_slots = max_seq_len - seq_len
    fragmented_memory = fragmented_slots * n_kv_heads * head_dim * 2 * 2 * n_layers  # bfloat16=2 bytes
    total_fragmented_memory += fragmented_memory

# -----------------------------
# æœ€çµ‚çµ±è¨ˆ
# -----------------------------
total_gb = total_fragmented_memory / 1e9
total_ratio = total_fragmented_memory / torch.cuda.get_device_properties(0).total_memory

print(f"\nğŸ“ˆ åš´æ ¼æ¸¬é‡çš„ Prompt Cache æ•ˆç›Šï¼š")
print(f"  ğŸ”¹ ç¸½ Prefill æ™‚é–“: {total_prefill_time:.3f} ç§’")
print(f"  ğŸ”¹ ç¸½ Cache æ¢å¾©æ™‚é–“: {total_restore_time:.3f} ç§’")
print(f"  ğŸ”¹ ç¸½ç¯€çœæ™‚é–“: {total_saved_time:.3f} ç§’")
print(f"  ğŸ”¹ æ•ˆèƒ½æå‡: {total_saved_time / (total_prefill_time + 1e-6) * 100:.1f}%")

print(f"\nâœ… ç¸½ç¢ç‰‡è¨˜æ†¶é«”: {total_gb:.2f} GB ({total_ratio * 100:.2f}%)")